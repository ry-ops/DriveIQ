# ===========================================
# DriveIQ Environment Configuration
# ===========================================

# Database (PostgreSQL with pgvector)
POSTGRES_USER=driveiq
POSTGRES_PASSWORD=driveiq
POSTGRES_DB=driveiq
DATABASE_URL=postgresql://driveiq:driveiq@postgres:5432/driveiq

# Redis (caching and session management)
REDIS_URL=redis://redis:6379/0

# Qdrant (vector similarity search)
QDRANT_HOST=qdrant
QDRANT_PORT=6333

# Anthropic API Key (required for cloud AI, optional for local LLM)
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# Security (CHANGE IN PRODUCTION)
SECRET_KEY=your-secret-key-for-jwt-tokens

# ===========================================
# Local LLM Configuration (Docker Model Runner)
# ===========================================
# Enable local LLM instead of cloud Anthropic API
# Set to true to use Docker Model Runner for inference
USE_LOCAL_LLM=false

# Custom API endpoint (Docker Model Runner or other Anthropic-compatible API)
# When USE_LOCAL_LLM=true, this should point to the model runner
ANTHROPIC_BASE_URL=http://model-runner:12434

# Local model to use (models available in Docker Model Runner)
# Options: ai/qwen3-coder, ai/glm-4.7-flash, ai/devstral-small-2, ai/gpt-oss
LOCAL_LLM_MODEL=ai/qwen3-coder

# ===========================================
# To use local LLM:
# 1. Start with: docker-compose --profile local-llm up -d
# 2. Set USE_LOCAL_LLM=true in your .env file
# 3. The first query will download the model (may take a few minutes)
# ===========================================

# ===========================================
# Optional: MCP Server Configuration
# ===========================================
# MCP_SERVER_PORT=8080
# MCP_AUTH_TOKEN=your-mcp-auth-token
